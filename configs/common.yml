task: classification
modelType: fasttext

# 注: 所有路径都需以项目为根目录写起.

# 路径相关参数
data_params:

  #是否使用预训练词向量
  use_embedding: true
  embedding_path: ./examples/classify/ChineseEmbedding.txt
  embed_dim: 200

  #停用词地址
  stopwords: []

  # 输出目录地址
  output_dir: ./examples/classify/politics_output
  #  最长句子长度
  max_sentence_len: 300
  #词汇表参数
  min_word_freq: 10 #词频最小值，大于该值才进入词汇表
  max_vocab_size: 20000  #词汇表最大词数
  #测试数据比例
  test_rate: 0.1
  #训练文件地址
  data_dir: ./examples/classify/politics_dataset
  data_file: ./examples/classify/politics_dataset/验证集.txt
  #提取词汇表目录
  data_vocab_dir: ./examples/classify/politics_dataset/politics
  data_vocab_tag: ['1']
  #是否句子拆分
  seg_sentence: false

# 模型参数
model_params:

  # 是否静态词向量
  is_static_word2vec: false
  #分类类别
  cate_list: ["0", "1"]
  # 卷积核数量
  kernel_num: 100
  #卷积核大小
  kernel_size_list: [3, 4, 5]
  dropout: 0.3
#训练参数
train_params:
  #学习率
  learning_rate: 0.001
  #训练轮数
  epoches: 6
  #是否使用GPU
  cuda: false
  log_interval: 10
  test_interval: 50
  save_interval: 100
  train_batch_size: 128
  test_batch_size: 100
  model_name: textcnnmodel
  continue_train: true
  is_valid_only: false

#xgboost参数列表
xgboost_params:
  learning_rate: 0.3
  # 树的个数--100棵树建立xgboost 总共迭代的次数
  n_estimators: 100
  # 树的深度
  max_depth: 6
  # 叶子节点最小权重
  min_child_weight: 1
  # 惩罚项中叶子结点个数前的参数
  gamma: 0.
  # 随机选择80%样本建立决策树
  subsample: 0.8
  # 随机选择80%特征建立决策树
  colsample_btree: 0.8
  # 指定损失函数
  objective: 'multi:softmax'
  # 解决样本个数不平衡的问题
  scale_pos_weight: 1
  # CPU个数，-1使用全部
  nthread: 2
  # 随机数
  random_state: 27
  # 分类数目
  num_class: 2
  eval_metric: "mlogloss"
  early_stopping_rounds: 10
  verbose: true
